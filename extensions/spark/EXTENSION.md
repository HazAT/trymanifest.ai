---
name: spark
version: 0.1.0
description: "Reactive AI sidekick for Manifest apps. Watches for errors and events, emits them to a file-based bus for consumption by a Pi agent."
author: "Manifest"
services:
  - spark: "Event emission, pause/resume control, and status reporting."
config:
  - SPARK_ENV: "Override environment (defaults to NODE_ENV, then 'development')."
  - enabled: "Master switch for Spark event emission. (default: true)"
  - eventsDir: "Directory for event files. (default: .spark/events)"
  - watch.unhandledErrors: "Capture uncaughtException/unhandledRejection. (default: true)"
  - watch.serverErrors: "Capture 500 errors from features. (default: true)"
  - environments.development.tools: "Tool access level in dev. (default: full)"
  - environments.development.behavior: "How Pi reacts to errors in dev. (default: fix)"
  - environments.production.tools: "Tool access level in prod. (default: readonly)"
  - environments.production.behavior: "How Pi reacts to errors in prod. (default: alert)"
  - pause.staleThresholdMinutes: "Minutes before a pause is considered stale. (default: 30)"
  - debounce.windowMs: "Debounce window for batching events. (default: 1000)"
---

# Spark

A reactive AI sidekick that watches your Manifest app for errors and injects them into a [Pi](https://github.com/nickarbon/pi-coding-agent) agent session. When your app throws a 500 or an unhandled exception, Spark captures the error with full context — feature name, route, stack trace, trace ID — and delivers it to Pi so it can investigate and (in development) fix the issue automatically.

---

## Getting Started

### 1. Initialize Spark

```bash
bun run manifest spark init
```

This does three things:
- Creates `config/spark.ts` with sensible defaults (if it doesn't exist)
- Adds the Spark Pi extension path to `.pi/settings.json`
- Adds process hooks to `index.ts` for uncaught errors

### 2. Set up Pi

**If you already have Pi installed and running in this project** — you're done. Restart Pi and Spark will auto-load via the extension registered in `.pi/settings.json`.

**If you don't have Pi yet:**

1. Install Pi globally:
   ```bash
   npm install -g @mariozechner/pi-coding-agent
   ```
2. You'll need an API key for your preferred LLM provider (Anthropic, OpenAI, etc.) — Pi uses your own subscription.
3. Run `pi` in the project directory. Spark will auto-load via `.pi/settings.json`.

### 3. Verify it works

1. Start your app: `bun --hot index.ts`
2. Trigger a 500 error (e.g., call a feature that hits a missing database)
3. Check that an event file appears in `.spark/events/`
4. If Pi is running, look for the error context appearing in your Pi session

---

## How It Works

Spark uses a **file-based event bus**. The architecture is deliberately simple — no sockets, no message queues, no dependencies.

1. **Your Manifest app** writes a JSON event file to `.spark/events/` whenever an error occurs. Writes are atomic (write to `.tmp`, then rename) to prevent partial reads.
2. **The Pi extension** watches `.spark/events/` using `fs.watch()`. When new files appear, it waits for a 1-second debounce window, then reads all pending events, batches them into a single message, and injects it into the Pi session.
3. **Trace IDs** link events back to requests. Server errors reuse the `request_id` from Manifest's response envelope. Unhandled errors get a fresh `crypto.randomUUID()`.

### Event format

```json
{
  "type": "server-error",
  "traceId": "abc-123-def",
  "timestamp": "2026-02-15T23:50:00.000Z",
  "environment": "development",
  "feature": "create-user",
  "route": "POST /api/users",
  "status": 500,
  "error": {
    "message": "Connection refused",
    "stack": "Error: Connection refused\n    at ..."
  },
  "request": {
    "input": { "email": "test@example.com" }
  }
}
```

Event types:
- **`server-error`** — a feature returned 500 or threw during `handle()`/`stream()`
- **`unhandled-error`** — `uncaughtException` or `unhandledRejection` at the process level

---

## Configuration

All configuration lives in `config/spark.ts`:

```typescript
export default {
  enabled: true,
  environment: (Bun.env.SPARK_ENV || Bun.env.NODE_ENV || 'development') as string,
  eventsDir: '.spark/events',

  watch: {
    unhandledErrors: true,
    serverErrors: true,
  },

  environments: {
    development: {
      tools: 'full' as const,     // Pi gets full coding tools
      behavior: 'fix' as const,   // Actively investigates and fixes errors
    },
    production: {
      tools: 'readonly' as const, // Pi can only read, not modify
      behavior: 'alert' as const, // Analyzes and reports, no code changes
    },
  },

  pause: {
    staleThresholdMinutes: 30,
  },

  debounce: {
    windowMs: 1000,
  },
}
```

- **`enabled`** — Master switch. When `false`, the Spark service is a no-op and `server.ts` skips event emission entirely (lazy import, zero cost).
- **`environment`** — Determines which environment config block applies. Reads `SPARK_ENV` first, then `NODE_ENV`, defaults to `'development'`.
- **`eventsDir`** — Where event files are written. Relative to project root.
- **`watch`** — Toggle which error types Spark captures.
- **`environments`** — Per-environment behavior. `tools` controls what Pi can do; `behavior` controls how aggressively it acts.
- **`pause.staleThresholdMinutes`** — If a pause file is older than this, it's considered stale and cleared on next Pi startup.
- **`debounce.windowMs`** — After the first new event file is detected, Spark waits this long before batching and delivering all pending events.

---

## Pause/Resume Protocol

Any agent (or human) can tell Spark to back off by creating a pause file:

```bash
bun run manifest spark pause "Deploying new migration, hold off"
```

This writes `.spark/pause` with:
```json
{
  "by": "cli",
  "since": "2026-02-15T23:55:00.000Z",
  "reason": "Deploying new migration, hold off"
}
```

While paused, the Pi extension **buffers events** instead of injecting them. On resume:

```bash
bun run manifest spark resume
```

The pause file is deleted, and Pi reviews any buffered events — acting on those still relevant and discarding stale ones.

**Check current state:**
```bash
bun run manifest spark status
```

Shows: enabled/disabled, current environment, pause state (with age), and pending event count.

**Stale pause detection:** If a pause file is older than `pause.staleThresholdMinutes` (default: 30 minutes), the Pi extension clears it on startup and runs a health assessment — checking for buffered events and reporting findings.

**Coordinating with other agents:** The pause file is a shared signal. If another tool or agent is actively working on the codebase (e.g., a CI pipeline or another Pi session), it can write the pause file to prevent Spark from interfering. The `by` field identifies who paused, and `reason` explains why.

---

## Environment Modes

### Development (`behavior: 'fix'`, `tools: 'full'`)

Pi has full access to coding tools. When an error arrives, Spark instructs Pi to:
1. Read the error and stack trace
2. Navigate to the relevant feature and source files
3. Investigate the root cause
4. Apply a fix if the cause is clear
5. Report what it found and what it changed

This is the default. Spark actively fixes your app while you work.

### Production (`behavior: 'alert'`, `tools: 'readonly'`)

Pi can only read files — no edits, no writes. When an error arrives, Spark instructs Pi to:
1. Analyze the error and stack trace
2. Identify the likely root cause
3. Report findings with suggested fixes
4. Wait for explicit human approval before any code changes

Use this for production monitoring. Spark becomes an observer that helps you understand what went wrong without touching anything.

---

## Troubleshooting

### Events not appearing in `.spark/events/`

1. Check that Spark is enabled:
   ```bash
   bun run manifest spark status
   ```
2. Verify `config/spark.ts` exists and `enabled` is `true`.
3. Check the `.spark/events/` directory exists:
   ```bash
   ls -la .spark/events/
   ```
   If missing, create it: `mkdir -p .spark/events`
4. Confirm the error type is being watched — check `watch.serverErrors` and `watch.unhandledErrors` in config.

### Pi not reacting to events

1. Verify the Pi extension is registered:
   ```bash
   cat .pi/settings.json
   ```
   Look for `"./extensions/spark/pi-extension"` in the `extensions` array.
2. Confirm Pi is running — run `pi` and look for "⚡ Spark" in the status area.
3. If Pi is running but not picking up events, check that `.spark/events/` is the correct path relative to the project root.

### Stale pause blocking events

1. Check pause status:
   ```bash
   bun run manifest spark status
   ```
2. If paused and you don't know why:
   ```bash
   cat .spark/pause
   ```
3. Clear it:
   ```bash
   bun run manifest spark resume
   ```

### Event files piling up and never consumed

This means the Pi extension isn't running or has crashed.
1. Check if Pi is running at all.
2. Check `.pi/settings.json` has the extension path.
3. Restart Pi: close and re-run `pi` in the project directory.
4. If events are very old (>5 min), the Pi extension cleans them on startup.

### Don't have Pi?

Install it globally:
```bash
npm install -g @mariozechner/pi-coding-agent
```
You'll need an API key for your preferred LLM provider (Anthropic, OpenAI, etc.). Run `pi` in the project directory — Spark will auto-load if `.pi/settings.json` is configured.
